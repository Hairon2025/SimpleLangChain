import getpass
import os
from typing import List, TypedDict  # ç”¨äºç±»å‹æ³¨è§£
from langchain_core.documents import Document  # LangChainæ–‡æ¡£ç±»å‹
from langchain_community.chat_models.tongyi import ChatTongyi  # é€šä¹‰åƒé—®èŠå¤©æ¨¡å‹
from langchain_community.embeddings import DashScopeEmbeddings  # é˜¿é‡Œäº‘DashScopeåµŒå…¥æ¨¡å‹
from langchain_community.document_loaders import WebBaseLoader  # ç½‘é¡µå†…å®¹åŠ è½½å™¨
from langchain_text_splitters import RecursiveCharacterTextSplitter  # æ–‡æœ¬åˆ†å‰²å™¨
from langchain_chroma import Chroma  # Chromaå‘é‡æ•°æ®åº“
from langgraph.graph import START, StateGraph  # LangGraphçŠ¶æ€å›¾
from langchain import hub  # LangChainæç¤ºè¯ä¸­å¿ƒ
import bs4  # ç½‘é¡µè§£æåº“

# è®¾ç½®DashScope APIå¯†é’¥ï¼ˆé˜¿é‡Œäº‘åƒé—®æ‰€éœ€ï¼‰
if not os.environ.get("DASHSCOPE_API_KEY"):
    # å¦‚æœç¯å¢ƒå˜é‡ä¸­æ²¡æœ‰å¯†é’¥ï¼Œé€šè¿‡å®‰å…¨è¾“å…¥è·å–
    os.environ["DASHSCOPE_API_KEY"] = getpass.getpass()

# åˆå§‹åŒ–é€šä¹‰åƒé—®å¤§æ¨¡å‹ï¼ˆä½¿ç”¨qwen-plusæ¨¡å‹ï¼‰
llm = ChatTongyi(model="qwen-plus")

# ==================== æ•°æ®åŠ è½½ä¸å¤„ç† ==================== 
# ä½¿ç”¨WebBaseLoaderåŠ è½½æŒ‡å®šç½‘é¡µå†…å®¹
loader = WebBaseLoader(
    web_paths=("https://lilianweng.github.io/posts/2023-06-23-agent/",),
    bs_kwargs=dict(
        # é€šè¿‡BeautifulSoupä»…è§£æç‰¹å®šclassçš„å†…å®¹ï¼ˆæé«˜æ•ˆç‡ï¼‰
        parse_only=bs4.SoupStrainer(
            class_=("post-content", "post-title", "post-header")
        )
    ),
)
docs = loader.load()  # æ‰§è¡ŒåŠ è½½æ“ä½œ

# ä½¿ç”¨é€’å½’å­—ç¬¦æ–‡æœ¬åˆ†å‰²å™¨å¤„ç†æ–‡æ¡£
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,  # æ¯ä¸ªæ–‡æœ¬å—å¤§å°
    chunk_overlap=200  # å—ä¹‹é—´é‡å å­—ç¬¦æ•°ï¼ˆä¿æŒä¸Šä¸‹æ–‡ï¼‰
)
all_splits = text_splitter.split_documents(docs)  # æ‰§è¡Œåˆ†å‰²æ“ä½œ

# ==================== å‘é‡å­˜å‚¨ä¸æ£€ç´¢ ==================== 
# åˆå§‹åŒ–DashScopeçš„æ–‡æœ¬åµŒå…¥æ¨¡å‹ï¼ˆç”¨äºç”Ÿæˆå‘é‡ï¼‰
embeddings = DashScopeEmbeddings(model="text-embedding-v2")

# åˆ›å»ºChromaå‘é‡æ•°æ®åº“å®ä¾‹
vector_store = Chroma(
    collection_name="example_collection",  # é›†åˆåç§°
    embedding_function=embeddings,  # ä½¿ç”¨çš„åµŒå…¥æ¨¡å‹
    persist_directory="./chroma_langchain_db"  # æœ¬åœ°æŒä¹…åŒ–ç›®å½•
)

# å°†åˆ†å‰²åçš„æ–‡æ¡£æ·»åŠ åˆ°å‘é‡æ•°æ®åº“
_ = vector_store.add_documents(documents=all_splits)

# ==================== æç¤ºè¯ä¸çŠ¶æ€å®šä¹‰ ==================== 
# ä»LangChain Hubè·å–é¢„å®šä¹‰çš„RAGæç¤ºè¯æ¨¡æ¿
prompt = hub.pull("rlm/rag-prompt")

# å®šä¹‰åº”ç”¨çš„çŠ¶æ€ç»“æ„ï¼ˆç±»å‹åŒ–å­—å…¸ï¼‰
class State(TypedDict):
    question: str    # ç”¨æˆ·é—®é¢˜
    context: List[Document]  # æ£€ç´¢åˆ°çš„æ–‡æ¡£ä¸Šä¸‹æ–‡
    answer: str      # ç”Ÿæˆçš„ç­”æ¡ˆ

# ==================== å®šä¹‰å¤„ç†æ­¥éª¤ ==================== 
def retrieve(state: State):
    """æ£€ç´¢é˜¶æ®µï¼šæ ¹æ®é—®é¢˜ä»å‘é‡åº“æŸ¥æ‰¾ç›¸å…³æ–‡æ¡£"""
    retrieved_docs = vector_store.similarity_search(state["question"])
    return {"context": retrieved_docs}

def generate(state: State):
    """ç”Ÿæˆé˜¶æ®µï¼šç»“åˆä¸Šä¸‹æ–‡ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ"""
    # å°†æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹åˆå¹¶
    docs_content = "\n\n".join(doc.page_content for doc in state["context"])
    
    # ä½¿ç”¨æç¤ºè¯æ¨¡æ¿æ ¼å¼åŒ–è¾“å…¥
    messages = prompt.invoke({
        "question": state["question"], 
        "context": docs_content
    })
    
    # è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå›ç­”
    response = llm.invoke(messages)
    return {"answer": response.content}

# ==================== æ„å»ºåº”ç”¨æµç¨‹å›¾ ==================== 
# åˆ›å»ºçŠ¶æ€å›¾å¹¶å®šä¹‰å¤„ç†æµç¨‹
graph_builder = StateGraph(State)
graph_builder.add_sequence([retrieve, generate])  # æ·»åŠ é¡ºåºæ‰§è¡ŒèŠ‚ç‚¹
graph_builder.add_edge(START, "retrieve")  # è®¾ç½®èµ·å§‹èŠ‚ç‚¹
graph = graph_builder.compile()  # ç¼–è¯‘æµç¨‹å›¾

# ==================== æµ‹è¯•åº”ç”¨ ==================== 
# è¾“å…¥é—®é¢˜å¹¶è·å–å›ç­”
# response = graph.invoke({"question": "ä»€ä¹ˆæ˜¯ReActï¼Ÿ"})
# print("æœ€ç»ˆç­”æ¡ˆï¼š", response["answer"])

# ==================== æµ‹è¯•åº”ç”¨ï¼šå¤šè½®æé—® ==================== 
print("\nğŸ” æ¬¢è¿ä½¿ç”¨ RAG åŠ©æ‰‹ï¼æˆ‘å¯ä»¥é€šè¿‡ç½‘é¡µå†…å®¹å›ç­”ä½ çš„é—®é¢˜ã€‚")
print("ğŸ“Œ å½“å‰çŸ¥è¯†åº“æ¥è‡ª: https://lilianweng.github.io/posts/2023-06-23-agent/")
print("ğŸ’¡ è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡ºï¼Œè¾“å…¥ 'clear' æ¸…é™¤ä¸Šä¸‹æ–‡ï¼ˆæ¯æ¬¡é—®é¢˜ç‹¬ç«‹æ£€ç´¢ï¼‰ã€‚\n")

while True:
    try:
        user_question = input("â“ è¯·è¾“å…¥ä½ çš„é—®é¢˜: ").strip()
        
        # é€€å‡ºå‘½ä»¤
        if user_question.lower() in ["quit", 'q', 'Q', "exit"]:
            print("ğŸ‘‹ å†è§ï¼")
            break
            
        # å¿½ç•¥ç©ºè¾“å…¥
        if not user_question:
            print("âš ï¸ è¯·è¾“å…¥æœ‰æ•ˆé—®é¢˜ã€‚\n")
            continue
            
        # è°ƒç”¨å›¾æ‰§è¡Œæµç¨‹ï¼ˆæ¯æ¬¡ç‹¬ç«‹æ£€ç´¢ + ç”Ÿæˆï¼‰
        print("ğŸ”„ æ­£åœ¨æ£€ç´¢ç›¸å…³ä¿¡æ¯...")
        response = graph.invoke({"question": user_question})
        answer = response["answer"]
        
        # è¾“å‡ºç»“æœ
        print(f"âœ… ç­”æ¡ˆ: {answer}\n")
        
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ è¢«åŠ¨ä¸­æ–­ï¼Œå†è§ï¼")
        break
    except Exception as e:
        print(f"âŒ æ‰§è¡Œå‡ºé”™: {e}\n")